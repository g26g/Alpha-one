{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n",
        "! wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n",
        "! pip install spectral\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHT73jN-fXa-",
        "outputId": "918aaf74-0f05-4f33-c3fe-a3bd772a4ed9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-17 02:49:24--  http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n",
            "Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat [following]\n",
            "--2022-11-17 02:49:24--  https://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5953527 (5.7M)\n",
            "Saving to: ‘Indian_pines_corrected.mat’\n",
            "\n",
            "Indian_pines_correc 100%[===================>]   5.68M  3.49MB/s    in 1.6s    \n",
            "\n",
            "2022-11-17 02:49:26 (3.49 MB/s) - ‘Indian_pines_corrected.mat’ saved [5953527/5953527]\n",
            "\n",
            "--2022-11-17 02:49:26--  http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n",
            "Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat [following]\n",
            "--2022-11-17 02:49:26--  https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1125 (1.1K)\n",
            "Saving to: ‘Indian_pines_gt.mat’\n",
            "\n",
            "Indian_pines_gt.mat 100%[===================>]   1.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-17 02:49:26 (66.7 MB/s) - ‘Indian_pines_gt.mat’ saved [1125/1125]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spectral) (1.21.6)\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.23.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "import spectral\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "dWBTLb-6Ckkg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_num = 16\n",
        "\n",
        "class HybridSN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(HybridSN, self).__init__()\n",
        "    self.conv1 = nn.Conv3d(1, 8, kernel_size=(7,3,3), stride=1, padding=0)\n",
        "    self.conv2 = nn.Conv3d(8, 16, kernel_size=(5,3,3), stride=1, padding=0)\n",
        "    self.conv3 = nn.Conv3d(16, 32, kernel_size=(3,3,3), stride=1, padding=0)\n",
        "    self.conv4 = nn.Conv2d(576, 64, kernel_size=(3,3), stride=1, padding=0)\n",
        "    self.fc1 = nn.Linear(18496, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 16)\n",
        "    self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(x)\n",
        "    x = x.reshape(x.shape[0],-1,19,19)\n",
        "    x = self.conv4(x)\n",
        "    x = F.relu(x)\n",
        "    x = x.flatten(start_dim = 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "JYs8WiK6CuaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #对高光谱数据 X 应用 PCA 变换\n",
        "def applyPCA(X, numComponents):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n",
        "    return newX\n",
        "\n",
        "# 对单个像素周围提取 patch 时，边缘像素就无法取了，因此，给这部分像素进行 padding 操作\n",
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX\n",
        "\n",
        "# 在每个像素周围提取 patch ，然后创建成符合 keras 处理的格式\n",
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
        "    # 给 X 做 padding\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n",
        "\n",
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "    \n"
      ],
      "metadata": {
        "id": "Tm4GQHZ66NDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 地物类别\n",
        "class_num = 16\n",
        "X = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "y = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
        "\n",
        "# 用于测试样本的比例\n",
        "test_ratio = 0.90\n",
        "# 每个像素周围提取 patch 的尺寸\n",
        "patch_size = 25\n",
        "# 使用 PCA 降维，得到主成分的数量\n",
        "pca_components = 30\n",
        "\n",
        "print('Hyperspectral data shape: ', X.shape)\n",
        "print('Label shape: ', y.shape)\n",
        "\n",
        "print('\\n... ... PCA tranformation ... ...')\n",
        "X_pca = applyPCA(X, numComponents=pca_components)\n",
        "print('Data shape after PCA: ', X_pca.shape)\n",
        "\n",
        "print('\\n... ... create data cubes ... ...')\n",
        "X_pca, y = createImageCubes(X_pca, y, windowSize=patch_size)\n",
        "print('Data cube X shape: ', X_pca.shape)\n",
        "print('Data cube y shape: ', y.shape)\n",
        "\n",
        "print('\\n... ... create train & test data ... ...')\n",
        "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X_pca, y, test_ratio)\n",
        "print('Xtrain shape: ', Xtrain.shape)\n",
        "print('Xtest  shape: ', Xtest.shape)\n",
        "\n",
        "# 改变 Xtrain, Ytrain 的形状，以符合 keras 的要求\n",
        "Xtrain = Xtrain.reshape(-1, patch_size, patch_size, pca_components, 1)\n",
        "Xtest  = Xtest.reshape(-1, patch_size, patch_size, pca_components, 1)\n",
        "print('before transpose: Xtrain shape: ', Xtrain.shape) \n",
        "print('before transpose: Xtest  shape: ', Xtest.shape) \n",
        "\n",
        "# 为了适应 pytorch 结构，数据要做 transpose\n",
        "Xtrain = Xtrain.transpose(0, 4, 3, 1, 2)\n",
        "Xtest  = Xtest.transpose(0, 4, 3, 1, 2)\n",
        "print('after transpose: Xtrain shape: ', Xtrain.shape) \n",
        "print('after transpose: Xtest  shape: ', Xtest.shape) \n",
        "\n",
        "\n",
        "\"\"\" Training dataset\"\"\"\n",
        "class TrainDS(torch.utils.data.Dataset): \n",
        "    def __init__(self):\n",
        "        self.len = Xtrain.shape[0]\n",
        "        self.x_data = torch.FloatTensor(Xtrain)\n",
        "        self.y_data = torch.LongTensor(ytrain)        \n",
        "    def __getitem__(self, index):\n",
        "        # 根据索引返回数据和对应的标签\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "    def __len__(self): \n",
        "        # 返回文件数据的数目\n",
        "        return self.len\n",
        "\n",
        "\"\"\" Testing dataset\"\"\"\n",
        "class TestDS(torch.utils.data.Dataset): \n",
        "    def __init__(self):\n",
        "        self.len = Xtest.shape[0]\n",
        "        self.x_data = torch.FloatTensor(Xtest)\n",
        "        self.y_data = torch.LongTensor(ytest)\n",
        "    def __getitem__(self, index):\n",
        "        # 根据索引返回数据和对应的标签\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "    def __len__(self): \n",
        "        # 返回文件数据的数目\n",
        "        return self.len\n",
        "\n",
        "# 创建 trainloader 和 testloader\n",
        "trainset = TrainDS()\n",
        "testset  = TestDS()\n",
        "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader  = torch.utils.data.DataLoader(dataset=testset,  batch_size=128, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txp6AHIk7Jfe",
        "outputId": "c8fe8e39-f730-47e9-f6d0-197099dcda1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperspectral data shape:  (145, 145, 200)\n",
            "Label shape:  (145, 145)\n",
            "\n",
            "... ... PCA tranformation ... ...\n",
            "Data shape after PCA:  (145, 145, 30)\n",
            "\n",
            "... ... create data cubes ... ...\n",
            "Data cube X shape:  (10249, 25, 25, 30)\n",
            "Data cube y shape:  (10249,)\n",
            "\n",
            "... ... create train & test data ... ...\n",
            "Xtrain shape:  (1024, 25, 25, 30)\n",
            "Xtest  shape:  (9225, 25, 25, 30)\n",
            "before transpose: Xtrain shape:  (1024, 25, 25, 30, 1)\n",
            "before transpose: Xtest  shape:  (9225, 25, 25, 30, 1)\n",
            "after transpose: Xtrain shape:  (1024, 1, 30, 25, 25)\n",
            "after transpose: Xtest  shape:  (9225, 1, 30, 25, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用GPU训练\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 网络放到GPU上\n",
        "net = HybridSN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# 开始训练\n",
        "total_loss = 0\n",
        "for epoch in range(300):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 优化器梯度归零\n",
        "        optimizer.zero_grad()\n",
        "        # 正向传播 +　反向传播 + 优化 \n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViLwyCnN7Qwl",
        "outputId": "215a2e26-9c8f-4a48-9987-9a6fa303b890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 1]   [loss avg: 20.2994]   [current loss: 2.3739]\n",
            "[Epoch: 2]   [loss avg: 19.6517]   [current loss: 2.2458]\n",
            "[Epoch: 3]   [loss avg: 18.9039]   [current loss: 1.9761]\n",
            "[Epoch: 4]   [loss avg: 18.0610]   [current loss: 1.8282]\n",
            "[Epoch: 5]   [loss avg: 17.0589]   [current loss: 1.3765]\n",
            "[Epoch: 6]   [loss avg: 16.1292]   [current loss: 1.2496]\n",
            "[Epoch: 7]   [loss avg: 15.0464]   [current loss: 0.6665]\n",
            "[Epoch: 8]   [loss avg: 13.9278]   [current loss: 0.8168]\n",
            "[Epoch: 9]   [loss avg: 12.8984]   [current loss: 0.4925]\n",
            "[Epoch: 10]   [loss avg: 12.0253]   [current loss: 0.7576]\n",
            "[Epoch: 11]   [loss avg: 11.2425]   [current loss: 0.4379]\n",
            "[Epoch: 12]   [loss avg: 10.5005]   [current loss: 0.2794]\n",
            "[Epoch: 13]   [loss avg: 9.8381]   [current loss: 0.2087]\n",
            "[Epoch: 14]   [loss avg: 9.2429]   [current loss: 0.1968]\n",
            "[Epoch: 15]   [loss avg: 8.7100]   [current loss: 0.2308]\n",
            "[Epoch: 16]   [loss avg: 8.2333]   [current loss: 0.2674]\n",
            "[Epoch: 17]   [loss avg: 7.8415]   [current loss: 0.0841]\n",
            "[Epoch: 18]   [loss avg: 7.4925]   [current loss: 0.1452]\n",
            "[Epoch: 19]   [loss avg: 7.1589]   [current loss: 0.0389]\n",
            "[Epoch: 20]   [loss avg: 6.8583]   [current loss: 0.1858]\n",
            "[Epoch: 21]   [loss avg: 6.5753]   [current loss: 0.1394]\n",
            "[Epoch: 22]   [loss avg: 6.3179]   [current loss: 0.0802]\n",
            "[Epoch: 23]   [loss avg: 6.0782]   [current loss: 0.0619]\n",
            "[Epoch: 24]   [loss avg: 5.8601]   [current loss: 0.1203]\n",
            "[Epoch: 25]   [loss avg: 5.6443]   [current loss: 0.0935]\n",
            "[Epoch: 26]   [loss avg: 5.4567]   [current loss: 0.1164]\n",
            "[Epoch: 27]   [loss avg: 5.2772]   [current loss: 0.0512]\n",
            "[Epoch: 28]   [loss avg: 5.1018]   [current loss: 0.0202]\n",
            "[Epoch: 29]   [loss avg: 4.9398]   [current loss: 0.0230]\n",
            "[Epoch: 30]   [loss avg: 4.7874]   [current loss: 0.0724]\n",
            "[Epoch: 31]   [loss avg: 4.6426]   [current loss: 0.0271]\n",
            "[Epoch: 32]   [loss avg: 4.5035]   [current loss: 0.0168]\n",
            "[Epoch: 33]   [loss avg: 4.3794]   [current loss: 0.0342]\n",
            "[Epoch: 34]   [loss avg: 4.2607]   [current loss: 0.0340]\n",
            "[Epoch: 35]   [loss avg: 4.1501]   [current loss: 0.0138]\n",
            "[Epoch: 36]   [loss avg: 4.0400]   [current loss: 0.0210]\n",
            "[Epoch: 37]   [loss avg: 3.9348]   [current loss: 0.0113]\n",
            "[Epoch: 38]   [loss avg: 3.8387]   [current loss: 0.0204]\n",
            "[Epoch: 39]   [loss avg: 3.7445]   [current loss: 0.0348]\n",
            "[Epoch: 40]   [loss avg: 3.6587]   [current loss: 0.0092]\n",
            "[Epoch: 41]   [loss avg: 3.5776]   [current loss: 0.0274]\n",
            "[Epoch: 42]   [loss avg: 3.5035]   [current loss: 0.0138]\n",
            "[Epoch: 43]   [loss avg: 3.4335]   [current loss: 0.0463]\n",
            "[Epoch: 44]   [loss avg: 3.3612]   [current loss: 0.0479]\n",
            "[Epoch: 45]   [loss avg: 3.2924]   [current loss: 0.0043]\n",
            "[Epoch: 46]   [loss avg: 3.2276]   [current loss: 0.1250]\n",
            "[Epoch: 47]   [loss avg: 3.1660]   [current loss: 0.0502]\n",
            "[Epoch: 48]   [loss avg: 3.1042]   [current loss: 0.0469]\n",
            "[Epoch: 49]   [loss avg: 3.0432]   [current loss: 0.0120]\n",
            "[Epoch: 50]   [loss avg: 2.9925]   [current loss: 0.0957]\n",
            "[Epoch: 51]   [loss avg: 2.9381]   [current loss: 0.0533]\n",
            "[Epoch: 52]   [loss avg: 2.8845]   [current loss: 0.0018]\n",
            "[Epoch: 53]   [loss avg: 2.8320]   [current loss: 0.0126]\n",
            "[Epoch: 54]   [loss avg: 2.7807]   [current loss: 0.0095]\n",
            "[Epoch: 55]   [loss avg: 2.7338]   [current loss: 0.0073]\n",
            "[Epoch: 56]   [loss avg: 2.6886]   [current loss: 0.0039]\n",
            "[Epoch: 57]   [loss avg: 2.6436]   [current loss: 0.0211]\n",
            "[Epoch: 58]   [loss avg: 2.5994]   [current loss: 0.0013]\n",
            "[Epoch: 59]   [loss avg: 2.5579]   [current loss: 0.0435]\n",
            "[Epoch: 60]   [loss avg: 2.5162]   [current loss: 0.0046]\n",
            "[Epoch: 61]   [loss avg: 2.4763]   [current loss: 0.0017]\n",
            "[Epoch: 62]   [loss avg: 2.4371]   [current loss: 0.0109]\n",
            "[Epoch: 63]   [loss avg: 2.3997]   [current loss: 0.0009]\n",
            "[Epoch: 64]   [loss avg: 2.3641]   [current loss: 0.0716]\n",
            "[Epoch: 65]   [loss avg: 2.3286]   [current loss: 0.0013]\n",
            "[Epoch: 66]   [loss avg: 2.2939]   [current loss: 0.0004]\n",
            "[Epoch: 67]   [loss avg: 2.2602]   [current loss: 0.0061]\n",
            "[Epoch: 68]   [loss avg: 2.2282]   [current loss: 0.0055]\n",
            "[Epoch: 69]   [loss avg: 2.1997]   [current loss: 0.1816]\n",
            "[Epoch: 70]   [loss avg: 2.1791]   [current loss: 0.2225]\n",
            "[Epoch: 71]   [loss avg: 2.1600]   [current loss: 0.2806]\n",
            "[Epoch: 72]   [loss avg: 2.1428]   [current loss: 0.1227]\n",
            "[Epoch: 73]   [loss avg: 2.1205]   [current loss: 0.0879]\n",
            "[Epoch: 74]   [loss avg: 2.0992]   [current loss: 0.0587]\n",
            "[Epoch: 75]   [loss avg: 2.0800]   [current loss: 0.0349]\n",
            "[Epoch: 76]   [loss avg: 2.0567]   [current loss: 0.0190]\n",
            "[Epoch: 77]   [loss avg: 2.0355]   [current loss: 0.0214]\n",
            "[Epoch: 78]   [loss avg: 2.0123]   [current loss: 0.0010]\n",
            "[Epoch: 79]   [loss avg: 1.9904]   [current loss: 0.0233]\n",
            "[Epoch: 80]   [loss avg: 1.9688]   [current loss: 0.0016]\n",
            "[Epoch: 81]   [loss avg: 1.9479]   [current loss: 0.0757]\n",
            "[Epoch: 82]   [loss avg: 1.9260]   [current loss: 0.0136]\n",
            "[Epoch: 83]   [loss avg: 1.9043]   [current loss: 0.0132]\n",
            "[Epoch: 84]   [loss avg: 1.8840]   [current loss: 0.0020]\n",
            "[Epoch: 85]   [loss avg: 1.8644]   [current loss: 0.1093]\n",
            "[Epoch: 86]   [loss avg: 1.8435]   [current loss: 0.0009]\n",
            "[Epoch: 87]   [loss avg: 1.8231]   [current loss: 0.0007]\n",
            "[Epoch: 88]   [loss avg: 1.8050]   [current loss: 0.0691]\n",
            "[Epoch: 89]   [loss avg: 1.7856]   [current loss: 0.0128]\n",
            "[Epoch: 90]   [loss avg: 1.7666]   [current loss: 0.0010]\n",
            "[Epoch: 91]   [loss avg: 1.7497]   [current loss: 0.1035]\n",
            "[Epoch: 92]   [loss avg: 1.7424]   [current loss: 0.1608]\n",
            "[Epoch: 93]   [loss avg: 1.7319]   [current loss: 0.1613]\n",
            "[Epoch: 94]   [loss avg: 1.7169]   [current loss: 0.0181]\n",
            "[Epoch: 95]   [loss avg: 1.7028]   [current loss: 0.0818]\n",
            "[Epoch: 96]   [loss avg: 1.6894]   [current loss: 0.0252]\n",
            "[Epoch: 97]   [loss avg: 1.6818]   [current loss: 0.1440]\n",
            "[Epoch: 98]   [loss avg: 1.6700]   [current loss: 0.2460]\n",
            "[Epoch: 99]   [loss avg: 1.6574]   [current loss: 0.0266]\n",
            "[Epoch: 100]   [loss avg: 1.6433]   [current loss: 0.0488]\n",
            "[Epoch: 101]   [loss avg: 1.6321]   [current loss: 0.0418]\n",
            "[Epoch: 102]   [loss avg: 1.6230]   [current loss: 0.0378]\n",
            "[Epoch: 103]   [loss avg: 1.6103]   [current loss: 0.0440]\n",
            "[Epoch: 104]   [loss avg: 1.5972]   [current loss: 0.0132]\n",
            "[Epoch: 105]   [loss avg: 1.5840]   [current loss: 0.0044]\n",
            "[Epoch: 106]   [loss avg: 1.5716]   [current loss: 0.0031]\n",
            "[Epoch: 107]   [loss avg: 1.5595]   [current loss: 0.0040]\n",
            "[Epoch: 108]   [loss avg: 1.5458]   [current loss: 0.0183]\n",
            "[Epoch: 109]   [loss avg: 1.5327]   [current loss: 0.0198]\n",
            "[Epoch: 110]   [loss avg: 1.5190]   [current loss: 0.0007]\n",
            "[Epoch: 111]   [loss avg: 1.5056]   [current loss: 0.0027]\n",
            "[Epoch: 112]   [loss avg: 1.4924]   [current loss: 0.0085]\n",
            "[Epoch: 113]   [loss avg: 1.4815]   [current loss: 0.0136]\n",
            "[Epoch: 114]   [loss avg: 1.4697]   [current loss: 0.0053]\n",
            "[Epoch: 115]   [loss avg: 1.4572]   [current loss: 0.0008]\n",
            "[Epoch: 116]   [loss avg: 1.4448]   [current loss: 0.0000]\n",
            "[Epoch: 117]   [loss avg: 1.4332]   [current loss: 0.0067]\n",
            "[Epoch: 118]   [loss avg: 1.4221]   [current loss: 0.0114]\n",
            "[Epoch: 119]   [loss avg: 1.4108]   [current loss: 0.0091]\n",
            "[Epoch: 120]   [loss avg: 1.4003]   [current loss: 0.1217]\n",
            "[Epoch: 121]   [loss avg: 1.3890]   [current loss: 0.0016]\n",
            "[Epoch: 122]   [loss avg: 1.3794]   [current loss: 0.1090]\n",
            "[Epoch: 123]   [loss avg: 1.3691]   [current loss: 0.0352]\n",
            "[Epoch: 124]   [loss avg: 1.3597]   [current loss: 0.0025]\n",
            "[Epoch: 125]   [loss avg: 1.3516]   [current loss: 0.0216]\n",
            "[Epoch: 126]   [loss avg: 1.3449]   [current loss: 0.0425]\n",
            "[Epoch: 127]   [loss avg: 1.3372]   [current loss: 0.1125]\n",
            "[Epoch: 128]   [loss avg: 1.3330]   [current loss: 0.3224]\n",
            "[Epoch: 129]   [loss avg: 1.3283]   [current loss: 0.0922]\n",
            "[Epoch: 130]   [loss avg: 1.3236]   [current loss: 0.1458]\n",
            "[Epoch: 131]   [loss avg: 1.3156]   [current loss: 0.0472]\n",
            "[Epoch: 132]   [loss avg: 1.3098]   [current loss: 0.0785]\n",
            "[Epoch: 133]   [loss avg: 1.3030]   [current loss: 0.0075]\n",
            "[Epoch: 134]   [loss avg: 1.2970]   [current loss: 0.0144]\n",
            "[Epoch: 135]   [loss avg: 1.2895]   [current loss: 0.0222]\n",
            "[Epoch: 136]   [loss avg: 1.2839]   [current loss: 0.0300]\n",
            "[Epoch: 137]   [loss avg: 1.2756]   [current loss: 0.0082]\n",
            "[Epoch: 138]   [loss avg: 1.2675]   [current loss: 0.0034]\n",
            "[Epoch: 139]   [loss avg: 1.2593]   [current loss: 0.0037]\n",
            "[Epoch: 140]   [loss avg: 1.2507]   [current loss: 0.0094]\n",
            "[Epoch: 141]   [loss avg: 1.2432]   [current loss: 0.0059]\n",
            "[Epoch: 142]   [loss avg: 1.2374]   [current loss: 0.0282]\n",
            "[Epoch: 143]   [loss avg: 1.2296]   [current loss: 0.0106]\n",
            "[Epoch: 144]   [loss avg: 1.2218]   [current loss: 0.0822]\n",
            "[Epoch: 145]   [loss avg: 1.2139]   [current loss: 0.0049]\n",
            "[Epoch: 146]   [loss avg: 1.2062]   [current loss: 0.0483]\n",
            "[Epoch: 147]   [loss avg: 1.1990]   [current loss: 0.0002]\n",
            "[Epoch: 148]   [loss avg: 1.1927]   [current loss: 0.0304]\n",
            "[Epoch: 149]   [loss avg: 1.1852]   [current loss: 0.0006]\n",
            "[Epoch: 150]   [loss avg: 1.1781]   [current loss: 0.0003]\n",
            "[Epoch: 151]   [loss avg: 1.1708]   [current loss: 0.0001]\n",
            "[Epoch: 152]   [loss avg: 1.1632]   [current loss: 0.0007]\n",
            "[Epoch: 153]   [loss avg: 1.1564]   [current loss: 0.0186]\n",
            "[Epoch: 154]   [loss avg: 1.1496]   [current loss: 0.0167]\n",
            "[Epoch: 155]   [loss avg: 1.1427]   [current loss: 0.0004]\n",
            "[Epoch: 156]   [loss avg: 1.1360]   [current loss: 0.0156]\n",
            "[Epoch: 157]   [loss avg: 1.1295]   [current loss: 0.0030]\n",
            "[Epoch: 158]   [loss avg: 1.1231]   [current loss: 0.0002]\n",
            "[Epoch: 159]   [loss avg: 1.1163]   [current loss: 0.0025]\n",
            "[Epoch: 160]   [loss avg: 1.1098]   [current loss: 0.0093]\n",
            "[Epoch: 161]   [loss avg: 1.1030]   [current loss: 0.0021]\n",
            "[Epoch: 162]   [loss avg: 1.0968]   [current loss: 0.0006]\n",
            "[Epoch: 163]   [loss avg: 1.0908]   [current loss: 0.0004]\n",
            "[Epoch: 164]   [loss avg: 1.0846]   [current loss: 0.0246]\n",
            "[Epoch: 165]   [loss avg: 1.0783]   [current loss: 0.0004]\n",
            "[Epoch: 166]   [loss avg: 1.0720]   [current loss: 0.0076]\n",
            "[Epoch: 167]   [loss avg: 1.0662]   [current loss: 0.0002]\n",
            "[Epoch: 168]   [loss avg: 1.0602]   [current loss: 0.0279]\n",
            "[Epoch: 169]   [loss avg: 1.0540]   [current loss: 0.0019]\n",
            "[Epoch: 170]   [loss avg: 1.0478]   [current loss: 0.0003]\n",
            "[Epoch: 171]   [loss avg: 1.0418]   [current loss: 0.0005]\n",
            "[Epoch: 172]   [loss avg: 1.0358]   [current loss: 0.0026]\n",
            "[Epoch: 173]   [loss avg: 1.0298]   [current loss: 0.0008]\n",
            "[Epoch: 174]   [loss avg: 1.0239]   [current loss: 0.0015]\n",
            "[Epoch: 175]   [loss avg: 1.0181]   [current loss: 0.0013]\n",
            "[Epoch: 176]   [loss avg: 1.0124]   [current loss: 0.0001]\n",
            "[Epoch: 177]   [loss avg: 1.0071]   [current loss: 0.0000]\n",
            "[Epoch: 178]   [loss avg: 1.0015]   [current loss: 0.0087]\n",
            "[Epoch: 179]   [loss avg: 0.9960]   [current loss: 0.0061]\n",
            "[Epoch: 180]   [loss avg: 0.9905]   [current loss: 0.0000]\n",
            "[Epoch: 181]   [loss avg: 0.9855]   [current loss: 0.0257]\n",
            "[Epoch: 182]   [loss avg: 0.9804]   [current loss: 0.0001]\n",
            "[Epoch: 183]   [loss avg: 0.9752]   [current loss: 0.0017]\n",
            "[Epoch: 184]   [loss avg: 0.9702]   [current loss: 0.0003]\n",
            "[Epoch: 185]   [loss avg: 0.9651]   [current loss: 0.0048]\n",
            "[Epoch: 186]   [loss avg: 0.9600]   [current loss: 0.0001]\n",
            "[Epoch: 187]   [loss avg: 0.9550]   [current loss: 0.0025]\n",
            "[Epoch: 188]   [loss avg: 0.9500]   [current loss: 0.0052]\n",
            "[Epoch: 189]   [loss avg: 0.9453]   [current loss: 0.0001]\n",
            "[Epoch: 190]   [loss avg: 0.9407]   [current loss: 0.0001]\n",
            "[Epoch: 191]   [loss avg: 0.9367]   [current loss: 0.0186]\n",
            "[Epoch: 192]   [loss avg: 0.9319]   [current loss: 0.0000]\n",
            "[Epoch: 193]   [loss avg: 0.9279]   [current loss: 0.0006]\n",
            "[Epoch: 194]   [loss avg: 0.9239]   [current loss: 0.0018]\n",
            "[Epoch: 195]   [loss avg: 0.9198]   [current loss: 0.0113]\n",
            "[Epoch: 196]   [loss avg: 0.9156]   [current loss: 0.0386]\n",
            "[Epoch: 197]   [loss avg: 0.9113]   [current loss: 0.0006]\n",
            "[Epoch: 198]   [loss avg: 0.9070]   [current loss: 0.0000]\n",
            "[Epoch: 199]   [loss avg: 0.9026]   [current loss: 0.0006]\n",
            "[Epoch: 200]   [loss avg: 0.8982]   [current loss: 0.0000]\n",
            "[Epoch: 201]   [loss avg: 0.8938]   [current loss: 0.0045]\n",
            "[Epoch: 202]   [loss avg: 0.8894]   [current loss: 0.0011]\n",
            "[Epoch: 203]   [loss avg: 0.8851]   [current loss: 0.0008]\n",
            "[Epoch: 204]   [loss avg: 0.8808]   [current loss: 0.0001]\n",
            "[Epoch: 205]   [loss avg: 0.8765]   [current loss: 0.0000]\n",
            "[Epoch: 206]   [loss avg: 0.8723]   [current loss: 0.0001]\n",
            "[Epoch: 207]   [loss avg: 0.8683]   [current loss: 0.0451]\n",
            "[Epoch: 208]   [loss avg: 0.8644]   [current loss: 0.0465]\n",
            "[Epoch: 209]   [loss avg: 0.8603]   [current loss: 0.0001]\n",
            "[Epoch: 210]   [loss avg: 0.8562]   [current loss: 0.0029]\n",
            "[Epoch: 211]   [loss avg: 0.8525]   [current loss: 0.0591]\n",
            "[Epoch: 212]   [loss avg: 0.8486]   [current loss: 0.0030]\n",
            "[Epoch: 213]   [loss avg: 0.8448]   [current loss: 0.0004]\n",
            "[Epoch: 214]   [loss avg: 0.8409]   [current loss: 0.0003]\n",
            "[Epoch: 215]   [loss avg: 0.8372]   [current loss: 0.0076]\n",
            "[Epoch: 216]   [loss avg: 0.8334]   [current loss: 0.0004]\n",
            "[Epoch: 217]   [loss avg: 0.8296]   [current loss: 0.0021]\n",
            "[Epoch: 218]   [loss avg: 0.8262]   [current loss: 0.0003]\n",
            "[Epoch: 219]   [loss avg: 0.8226]   [current loss: 0.0001]\n",
            "[Epoch: 220]   [loss avg: 0.8189]   [current loss: 0.0056]\n",
            "[Epoch: 221]   [loss avg: 0.8156]   [current loss: 0.0076]\n",
            "[Epoch: 222]   [loss avg: 0.8124]   [current loss: 0.0001]\n",
            "[Epoch: 223]   [loss avg: 0.8087]   [current loss: 0.0049]\n",
            "[Epoch: 224]   [loss avg: 0.8054]   [current loss: 0.0001]\n",
            "[Epoch: 225]   [loss avg: 0.8022]   [current loss: 0.0729]\n",
            "[Epoch: 226]   [loss avg: 0.7987]   [current loss: 0.0022]\n",
            "[Epoch: 227]   [loss avg: 0.7953]   [current loss: 0.0179]\n",
            "[Epoch: 228]   [loss avg: 0.7919]   [current loss: 0.0002]\n",
            "[Epoch: 229]   [loss avg: 0.7890]   [current loss: 0.0001]\n",
            "[Epoch: 230]   [loss avg: 0.7857]   [current loss: 0.0168]\n",
            "[Epoch: 231]   [loss avg: 0.7825]   [current loss: 0.0005]\n",
            "[Epoch: 232]   [loss avg: 0.7792]   [current loss: 0.0003]\n",
            "[Epoch: 233]   [loss avg: 0.7759]   [current loss: 0.0002]\n",
            "[Epoch: 234]   [loss avg: 0.7726]   [current loss: 0.0011]\n",
            "[Epoch: 235]   [loss avg: 0.7696]   [current loss: 0.0004]\n",
            "[Epoch: 236]   [loss avg: 0.7665]   [current loss: 0.0010]\n",
            "[Epoch: 237]   [loss avg: 0.7634]   [current loss: 0.0006]\n",
            "[Epoch: 238]   [loss avg: 0.7605]   [current loss: 0.0647]\n",
            "[Epoch: 239]   [loss avg: 0.7573]   [current loss: 0.0000]\n",
            "[Epoch: 240]   [loss avg: 0.7542]   [current loss: 0.0011]\n",
            "[Epoch: 241]   [loss avg: 0.7516]   [current loss: 0.0025]\n",
            "[Epoch: 242]   [loss avg: 0.7485]   [current loss: 0.0006]\n",
            "[Epoch: 243]   [loss avg: 0.7455]   [current loss: 0.0034]\n",
            "[Epoch: 244]   [loss avg: 0.7425]   [current loss: 0.0000]\n",
            "[Epoch: 245]   [loss avg: 0.7396]   [current loss: 0.0000]\n",
            "[Epoch: 246]   [loss avg: 0.7366]   [current loss: 0.0004]\n",
            "[Epoch: 247]   [loss avg: 0.7342]   [current loss: 0.0007]\n",
            "[Epoch: 248]   [loss avg: 0.7312]   [current loss: 0.0001]\n",
            "[Epoch: 249]   [loss avg: 0.7284]   [current loss: 0.0198]\n",
            "[Epoch: 250]   [loss avg: 0.7255]   [current loss: 0.0005]\n",
            "[Epoch: 251]   [loss avg: 0.7226]   [current loss: 0.0021]\n",
            "[Epoch: 252]   [loss avg: 0.7198]   [current loss: 0.0002]\n",
            "[Epoch: 253]   [loss avg: 0.7169]   [current loss: 0.0000]\n",
            "[Epoch: 254]   [loss avg: 0.7141]   [current loss: 0.0011]\n",
            "[Epoch: 255]   [loss avg: 0.7114]   [current loss: 0.0101]\n",
            "[Epoch: 256]   [loss avg: 0.7087]   [current loss: 0.0001]\n",
            "[Epoch: 257]   [loss avg: 0.7062]   [current loss: 0.0002]\n",
            "[Epoch: 258]   [loss avg: 0.7037]   [current loss: 0.0014]\n",
            "[Epoch: 259]   [loss avg: 0.7013]   [current loss: 0.0000]\n",
            "[Epoch: 260]   [loss avg: 0.6987]   [current loss: 0.0010]\n",
            "[Epoch: 261]   [loss avg: 0.6960]   [current loss: 0.0003]\n",
            "[Epoch: 262]   [loss avg: 0.6934]   [current loss: 0.0005]\n",
            "[Epoch: 263]   [loss avg: 0.6910]   [current loss: 0.0231]\n",
            "[Epoch: 264]   [loss avg: 0.6884]   [current loss: 0.0006]\n",
            "[Epoch: 265]   [loss avg: 0.6868]   [current loss: 0.0004]\n",
            "[Epoch: 266]   [loss avg: 0.6844]   [current loss: 0.0438]\n",
            "[Epoch: 267]   [loss avg: 0.6819]   [current loss: 0.0002]\n",
            "[Epoch: 268]   [loss avg: 0.6799]   [current loss: 0.0015]\n",
            "[Epoch: 269]   [loss avg: 0.6775]   [current loss: 0.0007]\n",
            "[Epoch: 270]   [loss avg: 0.6750]   [current loss: 0.0002]\n",
            "[Epoch: 271]   [loss avg: 0.6726]   [current loss: 0.0146]\n",
            "[Epoch: 272]   [loss avg: 0.6702]   [current loss: 0.0001]\n",
            "[Epoch: 273]   [loss avg: 0.6678]   [current loss: 0.0002]\n",
            "[Epoch: 274]   [loss avg: 0.6658]   [current loss: 0.0003]\n",
            "[Epoch: 275]   [loss avg: 0.6635]   [current loss: 0.0061]\n",
            "[Epoch: 276]   [loss avg: 0.6611]   [current loss: 0.0001]\n",
            "[Epoch: 277]   [loss avg: 0.6588]   [current loss: 0.0001]\n",
            "[Epoch: 278]   [loss avg: 0.6569]   [current loss: 0.0001]\n",
            "[Epoch: 279]   [loss avg: 0.6545]   [current loss: 0.0016]\n",
            "[Epoch: 280]   [loss avg: 0.6522]   [current loss: 0.0000]\n",
            "[Epoch: 281]   [loss avg: 0.6500]   [current loss: 0.0021]\n",
            "[Epoch: 282]   [loss avg: 0.6478]   [current loss: 0.0009]\n",
            "[Epoch: 283]   [loss avg: 0.6455]   [current loss: 0.0001]\n",
            "[Epoch: 284]   [loss avg: 0.6433]   [current loss: 0.0004]\n",
            "[Epoch: 285]   [loss avg: 0.6410]   [current loss: 0.0001]\n",
            "[Epoch: 286]   [loss avg: 0.6389]   [current loss: 0.0106]\n",
            "[Epoch: 287]   [loss avg: 0.6366]   [current loss: 0.0000]\n",
            "[Epoch: 288]   [loss avg: 0.6344]   [current loss: 0.0000]\n",
            "[Epoch: 289]   [loss avg: 0.6323]   [current loss: 0.0001]\n",
            "[Epoch: 290]   [loss avg: 0.6301]   [current loss: 0.0000]\n",
            "[Epoch: 291]   [loss avg: 0.6281]   [current loss: 0.0000]\n",
            "[Epoch: 292]   [loss avg: 0.6261]   [current loss: 0.0004]\n",
            "[Epoch: 293]   [loss avg: 0.6240]   [current loss: 0.0000]\n",
            "[Epoch: 294]   [loss avg: 0.6220]   [current loss: 0.0006]\n",
            "[Epoch: 295]   [loss avg: 0.6199]   [current loss: 0.0001]\n",
            "[Epoch: 296]   [loss avg: 0.6183]   [current loss: 0.0008]\n",
            "[Epoch: 297]   [loss avg: 0.6162]   [current loss: 0.0001]\n",
            "[Epoch: 298]   [loss avg: 0.6147]   [current loss: 0.0011]\n",
            "[Epoch: 299]   [loss avg: 0.6127]   [current loss: 0.0019]\n",
            "[Epoch: 300]   [loss avg: 0.6108]   [current loss: 0.0008]\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "# 模型测试\n",
        "for inputs, _ in test_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    outputs = net(inputs)\n",
        "    outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
        "    if count == 0:\n",
        "        y_pred_test =  outputs\n",
        "        count = 1\n",
        "    else:\n",
        "        y_pred_test = np.concatenate( (y_pred_test, outputs) )\n",
        "\n",
        "# 生成分类报告\n",
        "classification = classification_report(ytest, y_pred_test, digits=4)\n",
        "print(classification)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vhkP9Xa70RU",
        "outputId": "af310de0-7c3a-4549-b8ba-112102d090b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9286    0.9512    0.9398        41\n",
            "         1.0     0.9585    0.9533    0.9559      1285\n",
            "         2.0     0.9661    0.9906    0.9782       747\n",
            "         3.0     0.9901    0.9437    0.9663       213\n",
            "         4.0     0.9707    0.9885    0.9795       435\n",
            "         5.0     0.9632    0.9954    0.9790       657\n",
            "         6.0     1.0000    0.9200    0.9583        25\n",
            "         7.0     1.0000    1.0000    1.0000       430\n",
            "         8.0     0.8333    0.8333    0.8333        18\n",
            "         9.0     0.9717    0.9806    0.9761       875\n",
            "        10.0     0.9738    0.9760    0.9749      2210\n",
            "        11.0     0.9735    0.8951    0.9327       534\n",
            "        12.0     0.9887    0.9459    0.9669       185\n",
            "        13.0     0.9774    0.9860    0.9816      1139\n",
            "        14.0     0.9790    0.9395    0.9588       347\n",
            "        15.0     0.8316    0.9405    0.8827        84\n",
            "\n",
            "    accuracy                         0.9705      9225\n",
            "   macro avg     0.9566    0.9525    0.9540      9225\n",
            "weighted avg     0.9708    0.9705    0.9704      9225\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the original image\n",
        "X = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "y = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
        "\n",
        "height = y.shape[0]\n",
        "width = y.shape[1]\n",
        "\n",
        "X = applyPCA(X, numComponents= pca_components)\n",
        "X = padWithZeros(X, patch_size//2)\n",
        "\n",
        "# 逐像素预测类别\n",
        "outputs = np.zeros((height,width))\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        if int(y[i,j]) == 0:\n",
        "            continue\n",
        "        else :\n",
        "            image_patch = X[i:i+patch_size, j:j+patch_size, :]\n",
        "            image_patch = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1)\n",
        "            X_test_image = torch.FloatTensor(image_patch.transpose(0, 4, 3, 1, 2)).to(device)                                   \n",
        "            prediction = net(X_test_image)\n",
        "            prediction = np.argmax(prediction.detach().cpu().numpy(), axis=1)\n",
        "            outputs[i][j] = prediction+1\n",
        "    if i % 20 == 0:\n",
        "        print('... ... row ', i, ' handling ... ...')\n",
        "\n",
        "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "0xtW1H1S8rQe",
        "outputId": "7783e3d1-d288-48e3-9bba-eba3d10092e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... ... row  0  handling ... ...\n",
            "... ... row  20  handling ... ...\n",
            "... ... row  40  handling ... ...\n",
            "... ... row  60  handling ... ...\n",
            "... ... row  80  handling ... ...\n",
            "... ... row  100  handling ... ...\n",
            "... ... row  120  handling ... ...\n",
            "... ... row  140  handling ... ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}